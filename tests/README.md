# Tests

This directory contains the test suite for the `egohub` library.

## Philosophy

Our testing strategy follows the principles outlined in `testplan.md`. We use a combination of static analysis, unit tests, integration tests, and end-to-end tests to ensure the correctness and stability of the codebase.

- **Unit Tests**: These are fast, deterministic tests that check a single, isolated piece of functionality (e.g., a single function or class). They should not have external dependencies like network or disk I/O.
- **Integration Tests**: These tests verify that multiple components of the application work correctly together. For example, they might test the interaction between a `Dataset` and the HDF5 file it reads from. They are marked with `@pytest.mark.integration` and are not run by default with `make test`.
- **End-to-End (E2E) Tests**: These are the slowest and most comprehensive tests, checking a full user workflow (e.g., a CLI command). They are marked with `@pytest.mark.e2e`.

## How to Run Tests

The easiest way to run the core suite of fast unit tests is using the `Makefile` target:

```bash
# Activate the virtual environment first
source .venv/bin/activate

# Run fast unit tests
make test
```

To run the full suite, including slower integration tests, you can use `pytest` directly:

```bash
# Run all tests except performance benchmarks
pytest -m "not perf" -n auto
```

To run only a specific type of test:

```bash
# Run only integration tests
pytest -m "integration"
```

## Code Quality Checks

We use `ruff` for linting and `black` for code formatting:

```bash
# Run linting checks
ruff check .

# Auto-fix linting issues
ruff check . --fix

# Format code with black
black .

# Run both tests and linting
pytest && ruff check .
```

## Key Files & Directories

- `tests/conftest.py`: This file contains shared fixtures that can be used by any test in the suite. Most importantly, it houses the `hdf5_file_factory` fixture, which programmatically generates valid, temporary HDF5 files for testing data pipelines.
- `tests/data/`: This directory contains small, static test assets. Currently, it holds a `sample.mp4` video file used for testing video processing. Binary files here should be small and are tracked with Git LFS if necessary.
- `tests/test_*.py`: Individual test files, where each file generally corresponds to a module in the `egohub` library.
- `tests/test_constants/`: This directory contains snapshot files generated by `pytest-regressions`. You should not edit these files manually; they are created and updated by running the test suite.

## Test Categories

### Core Functionality Tests
- **Adapters** (`test_adapters.py`): Tests for EgoDex adapter functionality
- **Backends** (`test_backends.py`): Tests for HuggingFace and other backends
- **CLI** (`test_cli.py`): Tests for command-line interface
- **Datasets** (`test_datasets.py`): Tests for dataset loading and processing
- **Processing** (`test_processing.py`): Tests for skeleton and synchronization processing
- **Tasks** (`test_tasks.py`): Tests for object detection and pose estimation tasks
- **Transforms** (`test_transforms.py`): Tests for coordinate transformations

### Utility Tests
- **Constants** (`test_constants.py`): Tests for constant definitions
- **Coordinates** (`test_coordinates.py`): Tests for coordinate system utilities
- **Fixtures** (`test_fixtures.py`): Tests for test fixtures
- **Models** (`test_models.py`): Tests for model definitions
- **Objectives** (`test_objectives.py`): Tests for objective functions

## Writing New Tests

When adding a new feature, please also add corresponding tests.
- For pure functions, add a unit test.
- For features that interact with data files or other modules, add an integration test and use the available fixtures.
- Remember to mark slow tests with `@pytest.mark.integration` or `@pytest.mark.e2e` so they don't slow down the default test run.

### Test Guidelines
1. **Use descriptive test names** that explain what is being tested
2. **Follow the Arrange-Act-Assert pattern** for clear test structure
3. **Use fixtures** for common setup and teardown
4. **Mock external dependencies** to keep tests fast and reliable
5. **Test both success and failure cases**
6. **Keep tests independent** - each test should be able to run in isolation

## Troubleshooting

### Common Issues
- **Hanging tests**: Usually caused by infinite loops in mock setups or file operations
- **Mock issues**: Ensure mocks are properly configured for the expected behavior
- **Import errors**: Make sure all dependencies are installed in the virtual environment
- **Linting errors**: Run `ruff check . --fix` to auto-fix most issues

### Performance
- Unit tests should run in < 1 second each
- Integration tests should run in < 10 seconds each
- E2E tests may take longer but should be marked appropriately 